{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a97c763",
      "metadata": {
        "id": "9a97c763"
      },
      "source": [
        "### Librerie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd92f1be",
      "metadata": {
        "id": "dd92f1be"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb3b9ca",
      "metadata": {
        "id": "ceb3b9ca"
      },
      "source": [
        "##  Configurazioni iniziali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "899c8180",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "899c8180",
        "outputId": "6b0e8b02-8126-43d5-b516-f36036197a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a0067f0",
      "metadata": {
        "id": "9a0067f0"
      },
      "source": [
        "## Classe CLAHE per miglioramento immagini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "848568fc",
      "metadata": {
        "id": "848568fc"
      },
      "outputs": [],
      "source": [
        "class ApplyCLAHE:\n",
        "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
        "        self.clip_limit = clip_limit\n",
        "        self.tile_grid_size = tile_grid_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img_np = np.array(img)\n",
        "        img_lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
        "        l, a, b = cv2.split(img_lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
        "        l_clahe = clahe.apply(l)\n",
        "        img_lab_clahe = cv2.merge((l_clahe, a, b))\n",
        "        img_rgb_clahe = cv2.cvtColor(img_lab_clahe, cv2.COLOR_LAB2RGB)\n",
        "        return Image.fromarray(img_rgb_clahe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57afe53",
      "metadata": {
        "id": "b57afe53"
      },
      "source": [
        "## Trasformazioni per immagini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "feb99796",
      "metadata": {
        "id": "feb99796"
      },
      "outputs": [],
      "source": [
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomRotation(degrees=5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    ApplyCLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc1366d",
      "metadata": {
        "id": "8dc1366d"
      },
      "source": [
        "## Dataset multimodale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a2955015",
      "metadata": {
        "id": "a2955015"
      },
      "outputs": [],
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, tabular_data, labels, image_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tabular_data = torch.tensor(tabular_data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.df.iloc[idx]['ID']\n",
        "        image_path = os.path.join(self.image_dir, image_id + \".jpg\")\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        tabular = self.tabular_data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return image, tabular, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e38f923",
      "metadata": {
        "id": "5e38f923"
      },
      "source": [
        "##  Modello multimodale: ResNet18 + Tabular NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0a113134",
      "metadata": {
        "id": "0a113134"
      },
      "outputs": [],
      "source": [
        "class OptimizedMultimodalClassifier(nn.Module):\n",
        "    def __init__(self, tabular_input_dim):\n",
        "        super().__init__()\n",
        "        self.cnn = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "        self.cnn.fc = nn.Identity()\n",
        "        self.cnn_out_dim = 512\n",
        "        self.tabular_net = nn.Sequential(\n",
        "            nn.Linear(tabular_input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.cnn_out_dim + 64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, tabular):\n",
        "        image_feat = self.cnn(image)\n",
        "        tabular_feat = self.tabular_net(tabular)\n",
        "        combined = torch.cat([image_feat, tabular_feat], dim=1)\n",
        "        return self.classifier(combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b10abc49",
      "metadata": {
        "id": "b10abc49"
      },
      "source": [
        "## Preprocessing del CSV e delle feature (DALL'SVC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "194a4293",
      "metadata": {
        "id": "194a4293"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/mammografie.csv\")\n",
        "df = df.dropna(subset=['Severity', 'X', 'Y', 'Radius'])\n",
        "\n",
        "df['Target'] = df['Severity'].map({'B': 0, 'M': 1})\n",
        "df['X'] = pd.to_numeric(df['X'], errors='coerce')\n",
        "df['Y'] = pd.to_numeric(df['Y'], errors='coerce')\n",
        "df = df.dropna(subset=['X', 'Y'])\n",
        "\n",
        "df['ID_Num'] = df['ID'].str.extract(r'(\\d+)').astype(int)\n",
        "df['Area'] = np.pi * (df['Radius'] ** 2)\n",
        "df['DistanzaCentro'] = np.sqrt((df['X'] - 512)**2 + (df['Y'] - 512)**2)\n",
        "df['Lato'] = df['ID_Num'].apply(lambda n: 'sinistro' if n % 2 == 1 else 'destro')\n",
        "df['RadiusBin'] = pd.qcut(df['Radius'], q=5, labels=['XS', 'S', 'M', 'L', 'XL'])\n",
        "\n",
        "def quadrante(row):\n",
        "    if row['X'] < 512 and row['Y'] < 512: return 'Q0'\n",
        "    elif row['X'] >= 512 and row['Y'] < 512: return 'Q1'\n",
        "    elif row['X'] < 512 and row['Y'] >= 512: return 'Q2'\n",
        "    else: return 'Q3'\n",
        "\n",
        "df['Quadrante'] = df.apply(quadrante, axis=1)\n",
        "df['Quadrante_Lato'] = df['Quadrante'] + '_' + df['Lato']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3d08df",
      "metadata": {
        "id": "ad3d08df"
      },
      "source": [
        "##  Encoding e standardizzazione feature tabellari"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "10007545",
      "metadata": {
        "id": "10007545"
      },
      "outputs": [],
      "source": [
        "features = df[['Tissue', 'Class', 'Area', 'DistanzaCentro', 'Quadrante', 'RadiusBin']]\n",
        "labels = df['Target'].values\n",
        "\n",
        "numeric_features = ['Area', 'DistanzaCentro']\n",
        "categorical_features = ['Tissue', 'Class', 'Quadrante', 'RadiusBin']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "X_tabular = preprocessor.fit_transform(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd160e22",
      "metadata": {
        "id": "fd160e22"
      },
      "source": [
        "## Creazione dataset e dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4e5872d0",
      "metadata": {
        "id": "4e5872d0"
      },
      "outputs": [],
      "source": [
        "dataset = MultimodalDataset(\n",
        "    df, X_tabular, labels, \"/content/Mammografie\", transform=image_transform\n",
        ")\n",
        "\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "train_ds.dataset.transform = image_transform\n",
        "val_ds.dataset.transform = validation_image_transform\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efd49c4",
      "metadata": {
        "id": "5efd49c4"
      },
      "source": [
        "##  Inizializzazione modello e loss pesata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5c612ede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c612ede",
        "outputId": "ac42c2cf-023c-40e3-be10-17ec536693cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 144MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peso classi: tensor([0.8750, 1.1667])\n"
          ]
        }
      ],
      "source": [
        "model = OptimizedMultimodalClassifier(tabular_input_dim=X_tabular.shape[1]).to(device)\n",
        "\n",
        "class_counts = np.bincount(labels)\n",
        "total_samples = len(labels)\n",
        "class_weights = torch.tensor([\n",
        "    total_samples / (2.0 * class_counts[0]),\n",
        "    total_samples / (2.0 * class_counts[1])\n",
        "], dtype=torch.float32).to(device)\n",
        "\n",
        "print(f\"Peso classi: {class_weights}\")\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21531226",
      "metadata": {
        "id": "21531226"
      },
      "source": [
        "##  Training con early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "fc153f5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc153f5d",
        "outputId": "b6ae7278-2cd3-4b27-cf23-e5e3eed3bf51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "INIZIO...\n",
            "Epoch  1 | Train Loss: 0.7380 | Train Acc: 0.4458 | Val Loss: 0.6514 | Val Acc: 0.6389\n",
            "Epoch  2 | Train Loss: 0.5117 | Train Acc: 0.8313 | Val Loss: 0.7021 | Val Acc: 0.5000\n",
            "Epoch  3 | Train Loss: 0.4360 | Train Acc: 0.9639 | Val Loss: 0.7386 | Val Acc: 0.5000\n",
            "Epoch  4 | Train Loss: 0.4086 | Train Acc: 0.9518 | Val Loss: 0.7128 | Val Acc: 0.5000\n",
            "Epoch  5 | Train Loss: 0.3154 | Train Acc: 0.9880 | Val Loss: 0.7258 | Val Acc: 0.5278\n",
            "Epoch  6 | Train Loss: 0.2247 | Train Acc: 0.9880 | Val Loss: 0.7033 | Val Acc: 0.5556\n",
            "Epoch  7 | Train Loss: 0.2081 | Train Acc: 0.9880 | Val Loss: 0.7203 | Val Acc: 0.6111\n",
            "Epoch  8 | Train Loss: 0.1382 | Train Acc: 0.9880 | Val Loss: 0.6891 | Val Acc: 0.6389\n",
            "Early stopping at epoch 8\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "best_val_acc = 0\n",
        "patience = 7\n",
        "patience_counter = 0\n",
        "\n",
        "print(\"\\nINIZIO...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "    for images, tabular, labels_batch in train_loader:\n",
        "        images, tabular, labels_batch = images.to(device), tabular.to(device), labels_batch.to(device)\n",
        "        outputs = model(images, tabular)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels_batch).sum().item()\n",
        "        total_samples += labels_batch.size(0)\n",
        "\n",
        "    train_acc = correct / total_samples\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, tabular, labels_batch in val_loader:\n",
        "            images, tabular, labels_batch = images.to(device), tabular.to(device), labels_batch.to(device)\n",
        "            outputs = model(images, tabular)\n",
        "            loss = criterion(outputs, labels_batch)\n",
        "            val_loss += loss.item()\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_correct += (preds == labels_batch).sum().item()\n",
        "            val_total += labels_batch.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels_batch.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        best_preds = all_preds.copy()\n",
        "        best_labels = all_labels.copy()\n",
        "        best_probs = all_probs.copy()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19a6762",
      "metadata": {
        "id": "a19a6762"
      },
      "source": [
        "## ðŸ“ˆ Valutazione finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c9004e80",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9004e80",
        "outputId": "4c4fafbd-eddb-4d52-f381-b18c45849b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Final Results ===\n",
            "Best Validation Accuracy: 0.6389\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       0.73      0.44      0.55        18\n",
            "   Malignant       0.60      0.83      0.70        18\n",
            "\n",
            "    accuracy                           0.64        36\n",
            "   macro avg       0.66      0.64      0.62        36\n",
            "weighted avg       0.66      0.64      0.62        36\n",
            "\n",
            "\n",
            "AUC Score: 0.7068\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[ 8 10]\n",
            " [ 3 15]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n=== Final Results ===\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(best_labels, best_preds, target_names=[\"Benign\", \"Malignant\"]))\n",
        "\n",
        "if len(np.unique(best_labels)) > 1:\n",
        "    auc_score = roc_auc_score(best_labels, best_probs)\n",
        "    print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
        "\n",
        "print(\"\\n=== Confusion Matrix ===\")\n",
        "cm = confusion_matrix(best_labels, best_preds)\n",
        "print(cm)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}