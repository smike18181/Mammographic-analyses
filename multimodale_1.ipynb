{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d074ae6b",
   "metadata": {},
   "source": [
    "# Modello multimodale 1\n",
    "\n",
    "Questo modello si occupa di effettuare una classificazione multimodale utilizzando due feature di testo [type, breast] e le immagini per produrre una classificazione binaria: 0 se il tumore è benigno, 1 se il tumore è maligno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b05d265",
   "metadata": {},
   "source": [
    "## Import delle librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/PortableSSD/Università/FDSML/mias/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "# imposto i seed per la riproducibilità\n",
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83814a13",
   "metadata": {},
   "source": [
    "## Parametri e Costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'MIAS-JPEG'\n",
    "CSV_PATH = 'labels.csv'\n",
    "PATCH_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "NUM_AUG = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a110c1",
   "metadata": {},
   "source": [
    "## Data Augmentation e Preprocessing delle Immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b25a59c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alb_transform \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      3\u001b[0m     A\u001b[38;5;241m.\u001b[39mRotate(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, border_mode\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mBORDER_REFLECT_101, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      4\u001b[0m     A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m      5\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcrop_to_breast\u001b[39m(img, padding_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m):\n\u001b[1;32m      8\u001b[0m     blurred \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(img, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "alb_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "])\n",
    "\n",
    "def crop_to_breast(img, padding_ratio=0.05):\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return img\n",
    "    x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))\n",
    "    pad_w = int(w * padding_ratio)\n",
    "    pad_h = int(h * padding_ratio)\n",
    "    x1 = max(x - pad_w, 0)\n",
    "    y1 = max(y - pad_h, 0)\n",
    "    x2 = min(x + w + pad_w, img.shape[1])\n",
    "    y2 = min(y + h + pad_h, img.shape[0])\n",
    "    return img[y1:y2, x1:x2]\n",
    "\n",
    "def preprocess_image(path, augment=False):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Immagine non trovata o corrotta: {path}\")\n",
    "    if augment:\n",
    "        img = alb_transform(image=img)['image']\n",
    "    img = crop_to_breast(img)\n",
    "    img = 255 - img\n",
    "    img = cv2.resize(img, (PATCH_SIZE, PATCH_SIZE))\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img = clahe.apply(img)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e7c92",
   "metadata": {},
   "source": [
    "## Caricamento e Bilanciamento del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0cf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CSV_PATH)\n",
    "data = data[data['severity'].isin(['B', 'M'])]\n",
    "data['filename'] = data['filename'].astype(str).str.extract(r'(mdb\\d{3})')[0].str.strip()\n",
    "data = data.dropna(subset=['filename'])\n",
    "data['label'] = data['severity'].map({'B': 0, 'M': 1})\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "meta_features = ohe.fit_transform(data[['type', 'breast']])\n",
    "data_meta = pd.DataFrame(meta_features, columns=ohe.get_feature_names_out(['type', 'breast']))\n",
    "data = pd.concat([data.reset_index(drop=True), data_meta.reset_index(drop=True)], axis=1)\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42)\n",
    "\n",
    "maligni = train_df[train_df.label == 1]\n",
    "benigni = train_df[train_df.label == 0]\n",
    "train_df_balanced = pd.concat([benigni, maligni]).reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d76e6",
   "metadata": {},
   "source": [
    "## Generatori di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60aa2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(df, training=True):\n",
    "    meta_cols = ohe.get_feature_names_out(['type', 'breast'])\n",
    "    def generator():\n",
    "        for idx in range(len(df)):\n",
    "            row = df.iloc[idx]\n",
    "            img_path = os.path.join(IMG_DIR, f\"{row['filename']}.jpg\")\n",
    "            try:\n",
    "                label = row['label']\n",
    "                meta = row[meta_cols].values.astype(np.float32)\n",
    "                img = preprocess_image(img_path, augment=False)\n",
    "                yield {\"image\": img, \"meta\": meta}, label\n",
    "                if training:\n",
    "                    for _ in range(NUM_AUG):\n",
    "                        img_aug = preprocess_image(img_path, augment=True)\n",
    "                        yield {\"image\": img_aug, \"meta\": meta}, label\n",
    "            except Exception:\n",
    "                continue\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3978e44e",
   "metadata": {},
   "source": [
    "## Creazione Dataset TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f34252",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dim = len(ohe.get_feature_names_out(['type', 'breast']))\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    make_generator(train_df_balanced, training=True),\n",
    "    output_signature=(\n",
    "        {\n",
    "            \"image\": tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE, 1), dtype=tf.float32),\n",
    "            \"meta\": tf.TensorSpec(shape=(meta_dim,), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").shuffle(256).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    make_generator(val_df, training=False),\n",
    "    output_signature=(\n",
    "        {\n",
    "            \"image\": tf.TensorSpec(shape=(PATCH_SIZE, PATCH_SIZE, 1), dtype=tf.float32),\n",
    "            \"meta\": tf.TensorSpec(shape=(meta_dim,), dtype=tf.float32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int64)\n",
    "    )\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f871e",
   "metadata": {},
   "source": [
    "## Definizione del Modello CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ff7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    img_input = keras.Input(shape=(PATCH_SIZE, PATCH_SIZE, 1), name=\"image\")\n",
    "    meta_input = keras.Input(shape=(meta_dim,), name=\"meta\")\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(img_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    m = layers.Dense(32, activation='relu')(meta_input)\n",
    "    m = layers.Dropout(0.2)(m)\n",
    "    combined = layers.concatenate([x, m])\n",
    "    combined = layers.Dense(64, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.4)(combined)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(combined)\n",
    "    return keras.Model(inputs=[img_input, meta_input], outputs=outputs)\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=keras.losses.BinaryFocalCrossentropy(gamma=2),\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e6742",
   "metadata": {},
   "source": [
    "## Training del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a909b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 202ms/step - accuracy: 0.5311 - loss: 0.2013 - precision: 0.2307 - recall: 0.2385 - val_accuracy: 0.8056 - val_loss: 0.1653 - val_precision: 0.7222 - val_recall: 0.8667 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - accuracy: 0.5478 - loss: 0.1955 - precision: 0.2982 - recall: 0.4993 - val_accuracy: 0.4167 - val_loss: 0.2003 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - accuracy: 0.4846 - loss: 0.2000 - precision: 0.3402 - recall: 0.5501 - val_accuracy: 0.4167 - val_loss: 0.2528 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - accuracy: 0.5442 - loss: 0.1825 - precision: 0.4029 - recall: 0.6270 - val_accuracy: 0.4167 - val_loss: 0.3370 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 215ms/step - accuracy: 0.5775 - loss: 0.1710 - precision: 0.4699 - recall: 0.7298 - val_accuracy: 0.3889 - val_loss: 0.2707 - val_precision: 0.4000 - val_recall: 0.9333 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 201ms/step - accuracy: 0.5705 - loss: 0.1705 - precision: 0.4616 - recall: 0.6366 - val_accuracy: 0.4167 - val_loss: 0.4220 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.5870 - loss: 0.1679 - precision: 0.4759 - recall: 0.7479 - val_accuracy: 0.4167 - val_loss: 0.4301 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.6744 - loss: 0.1516 - precision: 0.5887 - recall: 0.6665 - val_accuracy: 0.4167 - val_loss: 0.4725 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 190ms/step - accuracy: 0.6280 - loss: 0.1628 - precision: 0.5670 - recall: 0.6433 - val_accuracy: 0.3889 - val_loss: 0.3497 - val_precision: 0.4000 - val_recall: 0.9333 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 204ms/step - accuracy: 0.6987 - loss: 0.1585 - precision: 0.6931 - recall: 0.6308 - val_accuracy: 0.4167 - val_loss: 0.3936 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 195ms/step - accuracy: 0.6925 - loss: 0.1480 - precision: 0.6492 - recall: 0.7199 - val_accuracy: 0.4167 - val_loss: 0.4433 - val_precision: 0.4167 - val_recall: 1.0000 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "classes = np.unique(train_df_balanced[\"label\"].values)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=classes, y=train_df_balanced[\"label\"].values)\n",
    "class_weights = dict(zip(classes, class_weights_array))\n",
    "\n",
    "steps_per_epoch = len(train_df_balanced) * (NUM_AUG + 1) // BATCH_SIZE\n",
    "val_steps = int(np.ceil(len(val_df) / BATCH_SIZE))\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bddbb",
   "metadata": {},
   "source": [
    "## Valutazione del Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b60bd88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Benigno       0.89      0.76      0.82        21\n",
      "     Maligno       0.72      0.87      0.79        15\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.81      0.81      0.80        36\n",
      "weighted avg       0.82      0.81      0.81        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/PortableSSD/Università/FDSML/mias/.venv/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(val_dataset).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_true = np.concatenate([y.numpy() for (_, _), y in val_dataset])\n",
    "print(classification_report(y_true, y_pred, target_names=['Benigno', 'Maligno']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
